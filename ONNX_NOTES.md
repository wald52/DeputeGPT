# üìù Notes sur l'Int√©gration ONNX de Mistral

## Situation Actuelle (D√©cembre 2025)

### Mod√®le Mistral-3B ONNX

Le mod√®le `Ministral-3-3B-Instruct-2512-ONNX` existe sur HuggingFace:
https://huggingface.co/mistralai/Ministral-3-3B-Instruct-2512-ONNX

**Fichiers disponibles:**
```
onnx/
‚îú‚îÄ‚îÄ decoder_model.onnx           (3.4 GB)
‚îú‚îÄ‚îÄ decoder_model_merged.onnx    (3.4 GB)
‚îú‚îÄ‚îÄ decoder_with_past_model.onnx (3.5 GB)
‚îî‚îÄ‚îÄ ...
```

### ‚ö†Ô∏è Probl√®me: Non compatible avec transformers.js

**Raison:** Ces fichiers ONNX bruts ne sont pas au format attendu par transformers.js v3.

transformers.js n√©cessite:
- `config.json` format√© sp√©cifiquement
- `tokenizer.json` au format Hugging Face
- Structure de fichiers sp√©cifique
- M√©tadonn√©es pour le pipeline

## Solutions Disponibles

### Solution 1: Utiliser un mod√®le d√©j√† converti ‚úÖ (Actuel)

```javascript
// Mod√®le compatible WebGPU d√©j√† dans transformers.js
generator = await pipeline(
    'text-generation',
    'onnx-community/Llama-3.2-1B-Instruct',
    { device: 'webgpu', dtype: 'q4' }
);
```

**Avantages:**
- Fonctionne imm√©diatement
- Optimis√© pour WebGPU
- Quantifi√© (rapide)

**Inconv√©nients:**
- Pas le mod√®le Mistral-3B exact
- Qualit√© peut varier

### Solution 2: Attendre la conversion officielle üïê

Mistral AI ou la communaut√© `onnx-community` convertiront probablement le mod√®le au format transformers.js dans les prochaines semaines.

**V√©rifier r√©guli√®rement:**
- https://huggingface.co/onnx-community
- https://github.com/xenova/transformers.js/discussions

### Solution 3: Conversion manuelle üîß (Avanc√©)

Si vous √™tes exp√©riment√© en Python/ONNX:

```bash
# 1. Installer les outils
pip install optimum transformers onnx onnxruntime

# 2. T√©l√©charger le mod√®le original
from transformers import AutoModelForCausalLM, AutoTokenizer
model = AutoModelForCausalLM.from_pretrained("mistralai/Ministral-3-3B-Instruct-2512")
tokenizer = AutoTokenizer.from_pretrained("mistralai/Ministral-3-3B-Instruct-2512")

# 3. Convertir en ONNX compatible transformers.js
from optimum.exporters.onnx import main_export
main_export(
    model_name_or_path="mistralai/Ministral-3-3B-Instruct-2512",
    output="./ministral-onnx-converted",
    task="text-generation-with-past"
)

# 4. Quantifier pour r√©duire la taille
from optimum.onnxruntime import ORTQuantizer
quantizer = ORTQuantizer.from_pretrained("./ministral-onnx-converted")
quantizer.quantize(save_dir="./ministral-q4", q_config="arm64")
```

**Note:** Ceci n√©cessite des connaissances techniques avanc√©es et peut ne pas garantir la compatibilit√© WebGPU.

## Mod√®les Alternatifs Recommand√©s

### Pour production imm√©diate:

1. **onnx-community/Llama-3.2-1B-Instruct** ‚úÖ
   - Taille: ~1 GB (q4)
   - Qualit√©: Bonne
   - WebGPU: Excellent

2. **onnx-community/Qwen2.5-1.5B-Instruct**
   - Taille: ~1.5 GB (q4)
   - Qualit√©: Tr√®s bonne
   - WebGPU: Excellent

3. **Felladrin/onnx-f16-mistral-7b-instruct-v0.1**
   - Taille: ~7 GB (fp16)
   - Qualit√©: Excellente
   - WebGPU: Bon (n√©cessite >8GB VRAM)

### Changer de mod√®le dans le code:

Dans `index.html`, ligne ~450:

```javascript
// Option 1: Petit et rapide (recommand√© pour d√©but)
generator = await pipeline('text-generation', 
    'onnx-community/Llama-3.2-1B-Instruct',
    { device: 'webgpu', dtype: 'q4' }
);

// Option 2: Balance qualit√©/taille
generator = await pipeline('text-generation',
    'onnx-community/Qwen2.5-1.5B-Instruct', 
    { device: 'webgpu', dtype: 'q4' }
);

// Option 3: Meilleure qualit√© (GPU puissant requis)
generator = await pipeline('text-generation',
    'Felladrin/onnx-f16-mistral-7b-instruct-v0.1',
    { device: 'webgpu', dtype: 'fp16' }
);
```

## Format des Prompts

Chaque mod√®le a son propre format. Adaptez dans `index.html`:

### Llama 3.2
```javascript
const messages = [
    { role: 'system', content: systemPrompt },
    { role: 'user', content: question }
];
// transformers.js g√®re automatiquement le template
```

### Mistral (quand disponible)
```javascript
const prompt = `<s>[INST] ${systemPrompt}

${question} [/INST]`;
```

## Quand Ministral-3B sera disponible

Surveillez ce repository: https://github.com/xenova/transformers.js

Ou testez avec:

```javascript
// Test de disponibilit√©
try {
    generator = await pipeline('text-generation',
        'onnx-community/Ministral-3-3B-Instruct',  // Nom hypoth√©tique
        { device: 'webgpu', dtype: 'q4' }
    );
    console.log('‚úÖ Ministral-3B disponible !');
} catch (error) {
    console.log('‚ö†Ô∏è Pas encore disponible, utilisation de Llama-3.2');
    generator = await pipeline('text-generation',
        'onnx-community/Llama-3.2-1B-Instruct',
        { device: 'webgpu', dtype: 'q4' }
    );
}
```

## Ressources

- [transformers.js v3 Documentation](https://huggingface.co/docs/transformers.js)
- [ONNX Community Models](https://huggingface.co/onnx-community)
- [Optimum ONNX Export](https://huggingface.co/docs/optimum/exporters/onnx/overview)
- [WebGPU Best Practices](https://tinyurl.com/webgpu-best-practices)

---

**TL;DR:** Pour l'instant, utilisez `Llama-3.2-1B-Instruct`. D√®s que Ministral-3B sera converti par la communaut√©, changez simplement le nom du mod√®le.
